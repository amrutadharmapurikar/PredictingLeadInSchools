{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aquatic-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import pickle\n",
    "import shapefile\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unique-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "this function removes the spaces from a given string and returns the new string\n",
    "'''\n",
    "def remove_spaces(string):\n",
    "    return string.replace(\" \", \"\")\n",
    "\n",
    "# tot_info.json contains the mapping of school address to its latitude and longitude\n",
    "# this info was gathered offline using Google Maps API\n",
    "\n",
    "f = open(\"/Users/23amrutad/Projects/LeadProj/ProjectData/tot_info.json\")\n",
    "lat_long = json.load(f)\n",
    "lat_long_map = {}\n",
    "for i in range(len(lat_long)):\n",
    "    val = lat_long[i]\n",
    "    key = remove_spaces(val[0].strip()[:-1])\n",
    "    value = [val[1], val[2]]\n",
    "    lat_long_map[key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "psychological-promise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in dataframe = 23255\n",
      "un problema ALSTEADPRIMARYSCHOOLALSTEADNH\n",
      "un problema AUBURNCHILDREN'SHOUSE,AMONTESSORISCHOOLAUBURNNH\n",
      "un problema CHARLESTOWNPRIMARYSCHOOLCHARLESTOWNNH\n",
      "un problema LITTLEHANDS,BIGDREAMSCHILDCARECENTERCONWAYNH\n",
      "un problema EPPINGELEMENTARYSCHOOL,SAUOFFICE,ANDPRESCHOOLEPPINGNH\n",
      "un problema JUSTLIKEHOMECHILDCAREMANCHESTERNH\n",
      "un problema GIRLSINCORPORATEDOFNEWHAMPSHIRE,MANCHESTERDIVISIONMANCHESTERNH\n",
      "un problema NORTHENDMONTESSORISCHOOL,LLCMANCHESTERNH\n",
      "un problema READY,SET...GROW!CHILDCARE&LEARNINGMARLOWNH\n",
      "un problema MONADNOCKREGIONALSCHOOLDISTRICT-ACES#93@MT.CAESARSWANZEYNH\n",
      "un problema HOMEAWAYFROMHOMEWALPOLENH\n",
      "un problema WALPOLEPRIMARYSCHOOLWALPOLENH\n"
     ]
    }
   ],
   "source": [
    "# NH_LEAD.csv contains official lead data published by state of NH, downloaded from <HERE>\n",
    "\n",
    "'''\n",
    "given a path, this function reads the csv file at the path\n",
    "'''\n",
    "def createLeadDict(path):\n",
    "    \"\"\"Read the CSV file and for every school store its name, address, \n",
    "        and the lowest lead concentration recorded at the school.\n",
    "        If the lead level is ND, ignore it.\n",
    "    \"\"\"\n",
    "    \n",
    "    def make_addr(dataframe, ind):\n",
    "        a = dataframe[\"Facility Name\"][ind]\n",
    "        b = dataframe[\"Town\"][ind]\n",
    "        c = dataframe[\"State\"][ind]\n",
    "        try:\n",
    "            address = a + \"  \" + b + \"  \" + c\n",
    "        except Exception as e:\n",
    "            print('unknown')\n",
    "            address = \"unknown\"\n",
    "        return address\n",
    "    \n",
    "    dataframe = pd.read_csv(path)\n",
    "    print('Number of rows in dataframe = %s'%len(dataframe))\n",
    "    name_map = {}\n",
    "    \n",
    "    for ind in dataframe.index:\n",
    "        name = dataframe[\"Facility Name\"][ind]\n",
    "        if name not in name_map.keys():\n",
    "            addr = make_addr(dataframe, ind)\n",
    "            name_map[name] = {'addr':addr, 'min_lead_quantity':-1, 'lat':'', 'long':'', 'tract':-1,\n",
    "                             'water_util_num_viol': -1, 'water_util_pop_size': -1}\n",
    "\n",
    "            \n",
    "    for k,v in name_map.items():\n",
    "        for ind in dataframe.index:\n",
    "            if k == dataframe[\"Facility Name\"][ind]:\n",
    "                try:\n",
    "                    if v['min_lead_quantity'] == -1 or float(dataframe[\"Result\"][ind]) < v['min_lead_quantity']:\n",
    "                        v['min_lead_quantity'] = float(dataframe[\"Result\"][ind])\n",
    "                except:\n",
    "                    pass\n",
    "    return name_map\n",
    "\n",
    "\n",
    "name_map = createLeadDict('/Users/23amrutad/Projects/LeadProj/ProjectData/NH_LEAD copy.csv')\n",
    "for k,v in name_map.items():\n",
    "    vnospace = remove_spaces(v['addr'])\n",
    "    if vnospace not in lat_long_map.keys():\n",
    "        print('un problema %s'%vnospace)\n",
    "    else:\n",
    "        lat_long = lat_long_map[vnospace]\n",
    "        name_map[k]['lat'] = lat_long[0]\n",
    "        name_map[k]['long'] = lat_long[1]\n",
    "        #print(lat_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "isolated-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "this function takes in the latitude and longitude of a school and returns the tract it's in\n",
    "'''\n",
    "\n",
    "def findTract(lat, long, shapes, records):\n",
    "    for i in range(len(shapes)):\n",
    "        pts = shapes[i].points\n",
    "        polygon = Polygon(pts)\n",
    "        #print(polygon)\n",
    "        pt = Point(long, lat)\n",
    "        if polygon.contains(pt):\n",
    "            res = records[i].TRACTCE\n",
    "            return res\n",
    "    return -1\n",
    "\n",
    "# finding the tract for each school in name_map\n",
    "\n",
    "# sf is the shapefile downloaded from the census website. it contains information about the boundaries of \n",
    "# the tracts from the 2019 5 yr ACS census\n",
    "sf = shapefile.Reader(\"/Users/23amrutad/Projects/LeadProj/ProjectData/cb_2019_33_tract_500k/cb_2019_33_tract_500k.shp\")\n",
    "shapes = sf.shapes()\n",
    "records = sf.records()\n",
    "\n",
    "# name_map should have the correct tract ID for each school, based on the data from shapes\n",
    "name_map_new = {}\n",
    "tracts = []\n",
    "for k, v in name_map.items():\n",
    "    if v['lat'] != '' and v['long'] != '':\n",
    "        v_new = v\n",
    "        tract_num = findTract(v['lat'], v['long'], shapes, records)\n",
    "        v_new['tract'] = tract_num\n",
    "        name_map_new[k] = v_new\n",
    "        tracts.append(v_new['tract'])\n",
    "name_map = name_map_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "widespread-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1 of data; includes the school name, address, tract number, latitude, longitude (doesn't include census data)\n",
    "# need to make further modifications, can restore initial data from here\n",
    "\n",
    "initial_path = \"/Users/23amrutad/Projects/LeadProj/ProjectData/initial_data.pkl\"\n",
    "initial = open(initial_path, 'wb')\n",
    "pickle.dump(name_map, initial)\n",
    "initial.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "original-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these files are downloaded from the Census Data API\n",
    "# the path_endings list contains the names of the tables downloaded\n",
    "# these files are read in as dataframes and merged together into one super dataframe with all the \n",
    "# necessary census data\n",
    "\n",
    "path_first = \"/Users/23amrutad/Projects/LeadProj/CensusFiles/\"\n",
    "path_endings = ['B01001', 'C17002', 'B14006', 'B17001', 'B06011', 'B11001', 'B19083', 'B25096', 'B15003', \n",
    "               'B25075', 'B14007', 'B19057', 'B25003', 'B25050', 'B25034', 'B09019']\n",
    "df = pd.read_csv(\"/Users/23amrutad/Projects/LeadProj/CensusFiles/B02001/B02001.csv\")\n",
    "for path in path_endings:\n",
    "    complete_path = path_first + path + \"/\" + path + \".csv\"\n",
    "    df1 = pd.read_csv(complete_path)\n",
    "    df = df.merge(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "norman-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "this function takes in a dictionary and a dataframe,\n",
    "and returns the row in the dataframe the corresponds to an entry in the dictinoary\n",
    "\n",
    "specificially, the dataframe is census data and the dictionary contains the details of the school,\n",
    "the tract number from the dicitionary is used to determine whether the school is actually in the dataframe.\n",
    "if it is, all the info is returned\n",
    "'''\n",
    "\n",
    "def returnCensusInfo(val : dict, census_df):\n",
    "    target_tract = val['tract']\n",
    "    \n",
    "    for r in census_df.iterrows():\n",
    "        # extract tract id from the row\n",
    "        t = r[1]['GEO_ID']\n",
    "        if t == 'id':\n",
    "            continue\n",
    "        #print(\"%s ==? %s\"%(t[14:], target_tract))\n",
    "        if len(t) > 14 and t[14:] == target_tract:\n",
    "            # only when the lower 6 characters of the tract match, then copy\n",
    "            # the value of each attribute\n",
    "            features = (r[1].keys().tolist())\n",
    "            for x in features:\n",
    "                val[x] = r[1].get(x)\n",
    "            break\n",
    "    return val\n",
    "\n",
    "# loading the previously saved data to combine it with the census data\n",
    "a = open(initial_path, 'rb')\n",
    "name_map = pickle.load(a)\n",
    "\n",
    "name_map_copy = {}\n",
    "for k, v in name_map.items():\n",
    "    newinfo = returnCensusInfo(v, df)\n",
    "    if newinfo is not None:\n",
    "        name_map_copy[k] = newinfo\n",
    "        \n",
    "name_map_df = pd.DataFrame.from_dict(name_map)\n",
    "name_map = name_map_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "controversial-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting every value in name_map to float\n",
    "\n",
    "for r in name_map.keys():\n",
    "    v = name_map[r]\n",
    "    for k in v.keys():\n",
    "        try:\n",
    "            v[k] = float(v[k])\n",
    "        except:\n",
    "            v[k] = 0.0\n",
    "    name_map[r] = v\n",
    "name_map_df = pd.DataFrame.from_dict(name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nearby-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the updated, fully populated, and float-only name_map to a pickle file.\n",
    "# also saving a version as a spreadsheet, for visual inspection.\n",
    "\n",
    "dbfile = open(\"/Users/23amrutad/Projects/LeadProj/ProjectData/final_data.pkl\", 'wb')\n",
    "pickle.dump(name_map, dbfile)\n",
    "dbfile.close()\n",
    "name_map_df.to_excel(\"/Users/23amrutad/Projects/LeadProj/ProjectData/final_data.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
